{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Алгоритмы интеллектуальной обработки больших объемов данных\n",
    "## Домашнее задание №2: Линейные модели\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### <hr\\>\n",
    "**Общая информация**\n",
    "\n",
    "**Срок сдачи:** 09 ноября 18:00 Сдача **очная** на онлайн занятии. <br\\>\n",
    "\n",
    "\n",
    "Используйте данный Ipython Notebook при оформлении домашнего задания.\n",
    "\n",
    "Присылать ДЗ необходимо в виде ссылки на свой github репозиторий на почту ml1.sphere@mail.ru с указанием темы в следующем формате:\n",
    "\n",
    "[ML0920, Задание 2] Фамилия Имя.\n",
    "\n",
    "\n",
    "\n",
    "**Штрафные баллы:**\n",
    "\n",
    "1. Невыполнение PEP8 -1 балл\n",
    "2. Отсутствие фамилии в имени скрипта (скрипт должен называться по аналогии со stroykova_hw2.ipynb) -1 балл\n",
    "3. Все строчки должны быть выполнены. Нужно, чтобы output команды можно было увидеть уже в git'е. В противном случае -1 балл\n",
    "4. При оформлении ДЗ нужно пользоваться данным файлом в качестве шаблона. Не нужно удалять и видоизменять написанный код и текст, если явно не указана такая возможность. В противном случае -1 балл\n",
    "<hr\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здравствуйте, уважаемые студенты! \n",
    "\n",
    "В этом задании мы будем реализовать линейные модели. Необходимо реализовать линейную и логистическую регрессии с L2 регуляризацией"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теоретическое введение\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Линейная регрессия решает задачу регрессии и оптимизирует функцию потерь MSE \n",
    "\n",
    "$$L(w) =  \\frac{1}{N}\\left[\\sum_i (y_i - a_i) ^ 2 \\right], $$ где $y_i$ $-$ целевая функция,  $a_i = a(x_i) =  \\langle\\,x_i,w\\rangle ,$ $-$ предсказание алгоритма на объекте $x_i$, $w$ $-$ вектор весов (размерности $D$), $x_i$ $-$ вектор признаков (такой же размерности $D$).\n",
    "\n",
    "Не забываем, что здесь и далее  мы считаем, что в $x_i$ есть тождественный вектор единиц, ему соответствует вес $w_0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия является линейным классификатором, который оптимизирует так называемый функционал log loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L(w) = - \\frac{1}{N}\\left[\\sum_i y_i \\log a_i + ( 1 - y_i) \\log (1 - a_i) \\right],$$\n",
    "где  $y_i  \\in \\{0,1\\}$ $-$ метка класса, $a_i$ $-$ предсказание алгоритма на объекте $x_i$. Модель пытается предсказать апостериорую вероятность объекта принадлежать к классу \"1\":\n",
    "$$ p(y_i = 1 | x_i) = a(x_i) =  \\sigma( \\langle\\,x_i,w\\rangle ),$$\n",
    "$w$ $-$ вектор весов (размерности $D$), $x_i$ $-$ вектор признаков (такой же размерности $D$).\n",
    "\n",
    "Функция $\\sigma(x)$ $-$ нелинейная функция, пероводящее скалярное произведение объекта на веса в число $\\in (0,1)$ (мы же моделируем вероятность все-таки!)\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + \\exp(-x)}$$\n",
    "\n",
    "Если внимательно посмотреть на функцию потерь, то можно заметить, что в зависимости от правильного ответа алгоритм штрафуется или функцией $-\\log a_i$, или функцией $-\\log (1 - a_i)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто для решения проблем, которые так или иначе связаны с проблемой переобучения, в функционал качества добавляют слагаемое, которое называют ***регуляризацией***. Итоговый функционал для линейной регрессии тогда принимает вид:\n",
    "\n",
    "$$L(w) =  \\frac{1}{N}\\left[\\sum_i (y_i - a_i) ^ 2 \\right] + \\frac{1}{C}R(w) $$\n",
    "\n",
    "Для логистической: \n",
    "$$L(w) = - \\frac{1}{N}\\left[\\sum_i y_i \\log a_i + ( 1 - y_i) \\log (1 - a_i) \\right] +  \\frac{1}{C}R(w)$$\n",
    "\n",
    "Самое понятие регуляризации введено основателем ВМК академиком Тихоновым https://ru.wikipedia.org/wiki/Метод_регуляризации_Тихонова\n",
    "\n",
    "Идейно методика регуляризации заключается в следующем $-$ мы рассматриваем некорректно поставленную задачу (что это такое можно найти в интернете), для того чтобы сузить набор различных вариантов (лучшие из которых будут являться переобучением ) мы вводим дополнительные ограничения на множество искомых решений. На лекции Вы уже рассмотрели два варианта регуляризации.\n",
    "\n",
    "$L1$ регуляризация:\n",
    "$$R(w) = \\sum_{j=1}^{D}|w_j|$$\n",
    "$L2$ регуляризация:\n",
    "$$R(w) =  \\sum_{j=1}^{D}w_j^2$$\n",
    "\n",
    "С их помощью мы ограничиваем модель в  возможности выбора каких угодно весов минимизирующих наш лосс, модель уже не сможет подстроиться под данные как ей угодно. \n",
    "\n",
    "Вам нужно добавить соотвествущую Вашему варианту $L2$ регуляризацию.\n",
    "\n",
    "И так, мы поняли, какую функцию ошибки будем минимизировать, разобрались, как получить предсказания по объекту и обученным весам. Осталось разобраться, как получить оптимальные веса. Для этого нужно выбрать какой-то метод оптимизации.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентный спуск является самым популярным алгоритмом обучения линейных моделей. В этом задании Вам предложат реализовать стохастический градиентный спуск или  мини-батч градиентный спуск (мини-батч на русский язык довольно сложно перевести, многие переводят это как \"пакетный\", но мне не кажется этот перевод удачным). Далее нам потребуется определение **эпохи**.\n",
    "Эпохой в SGD и MB-GD называется один проход по **всем** объектам в обучающей выборки.\n",
    "* В SGD градиент расчитывается по одному случайному объекту. Сам алгоритм выглядит примерно так:\n",
    "        1) Перемешать выборку\n",
    "        2) Посчитать градиент функции потерь на одном объекте (далее один объект тоже будем называть батчем)\n",
    "        3) Сделать шаг спуска\n",
    "        4) Повторять 2) и 3) пока не пройдет максимальное число эпох.\n",
    "* В Mini Batch SGD - по подвыборке объектов. Сам алгоритм выглядит примерно так::\n",
    "        1) Перемешать выборку, выбрать размер мини-батча (от 1 до размера выборки)\n",
    "        2) Почитать градиент функции потерь по мини-батчу (не забыть поделить на  число объектов в мини-батче)\n",
    "        3) Сделать шаг спуска\n",
    "        4) Повторять 2) и 3) пока не пройдет максимальное число эпох.\n",
    "* Для отладки алгоритма реализуйте возможность  вывода средней ошибки на обучении модели по объектам (мини-батчам). После шага градиентного спуска посчитайте значение ошибки на объекте (или мини-батче), а затем усредните, например, по ста шагам. Если обучение проходит корректно, то мы должны увидеть, что каждые 100 шагов функция потерь уменьшается. \n",
    "* Правило останова - максимальное количество эпох\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теоретические вопросы (2 балла)\n",
    "В этой части Вам будут предложены теоретичские вопросы и задачи по теме. Вы, конечно, можете списать их у своего товарища или найти решение в интернете, но учтите, что они обязательно войдут в теоретический коллоквиум. Лучше разобраться в теме сейчас и успешно ответить на коллоквиуме, чем списать, не разобравшись в материале, и быть терзаемым совестью. \n",
    "\n",
    "\n",
    "Формулы надо оформлять в формате **LaTeX**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача 1. Градиент для линейной регрессии.\n",
    "* Выпишите формулу обновления весов для линейной регрессии с L2 регуляризацией для мини-батч градиентого спуска размера $n$:\n",
    "\n",
    "$$ w_{new} = w_{old} - ... $$\n",
    "\n",
    " Отнеситесь к этому пункту максимально серьезно, это Вам нужно будет реализовать в задании.\n",
    " \n",
    "Проанализруйте итоговую формулу градиента - как  интуитивно можно  описать, чему равен градиент?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ваше решение здесь***\n",
    "\n",
    "Как известно, формула для градиентного спуска имеет вид $w_{new} = w_{old} - \\alpha_kgrad(L(w))$. Посчитаем градиент функции потерь.\n",
    "\n",
    "$$grad(L(w)) = grad\\left(\\frac{1}{n}\\left[\\sum_{i=1}^n (\\langle x_i, w_{old}\\rangle - y_i) ^ 2 \\right] + \\frac{1}{2C}|w_{old}|^2\\right) = \\frac{1}{n}\\left[\\sum_{i=1}^n 2x_i(\\langle x_i, w_{old}\\rangle - y_i)\\right] + \\frac{1}{C}w_{old}$$\n",
    "\n",
    "Подставим теперь это выражение в исходную формулу и получим ответ.\n",
    "\n",
    "$$w_{new} = w_{old} - \\alpha_k\\left(\\frac{1}{n}\\left[\\sum_{i=1}^n 2x_i(\\langle x_i, w_{old}\\rangle - y_i)\\right] + \\frac{1}{C}w_{old}\\right)$$\n",
    "\n",
    "Из формулы видно, что градиент функции потерь пропорционален ошибке на текущем шаге градиентного спуска."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача 2. Градиент для логистической регрессии.\n",
    "* Выпишите формулу обновления весов для логистической регрессии с L2 регуляризацией  для мини-батч градиентого спуска размера $n$:\n",
    "\n",
    "$$ w_{new} = w_{old} - ... $$\n",
    "\n",
    " Отнеситесь к этому пункту максимально серьезно, это Вам нужно будет реализовать в задании.\n",
    " \n",
    "Проанализруйте итоговую формулу градиента - как  интуитивно можно  описать, чему равен градиент? Как соотносится этот градиент с градиентом, возникающий в задаче линейной регрессии?\n",
    "\n",
    "Подсказка: Вам градиент, которой получается если “в лоб” продифференцировать,  надо немного преобразовать.\n",
    "Надо подставить, что $1 - \\sigma(w,x) $ это  $1 - a(x_i)$, а  $-\\sigma(w,x)$ это $0 - a(x_i)$.  Тогда получится свести к одной красивой формуле с линейной регрессией, которую программировать будет намного проще."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ваше решение здесь***\n",
    "\n",
    "Как и в предыдущей задаче посчитаем градиент функции потерь, а потом подставим его в формулу обновления весов.\n",
    "\n",
    "$$grad(L(w)) = grad\\left(- \\frac{1}{n}\\left[\\sum_{i=1}^n y_i \\log \\sigma (\\langle x_i, w_{old}\\rangle) + (1 - y_i) \\log (1 - \\sigma (\\langle x_i, w_{old}\\rangle)) \\right] +  \\frac{1}{2C}|w_{old}|^2\\right) =$$\n",
    "\n",
    "$$= - \\frac{1}{n}\\left[\\sum_{i=1}^n y_i\\frac{\\sigma' (\\langle x_i, w_{old}\\rangle)}{\\sigma (\\langle x_i, w_{old}\\rangle)}x_i - (1 - y_i)\\frac{\\sigma' (\\langle x_i, w_{old}\\rangle)}{1 - \\sigma (\\langle x_i, w_{old}\\rangle)}x_i\\right] + \\frac{1}{C}w_{old}$$\n",
    "\n",
    "Посчитаем производную сигмоиды.\n",
    "\n",
    "$$\\sigma'(x) = \\left(\\frac{1}{1 + e^{-x}}\\right)' = \\frac{e^{-x}}{(1 + e^{-x})^2} = e^{-x}\\sigma^2(x)$$\n",
    "\n",
    "С учётом этого преобразуем градиент.\n",
    "\n",
    "$$- \\frac{1}{n}\\left[\\sum_{i=1}^n y_i\\frac{\\sigma' (\\langle x_i, w_{old}\\rangle)}{\\sigma (\\langle x_i, w_{old}\\rangle)}x_i - (1 - y_i)\\frac{\\sigma' (\\langle x_i, w_{old}\\rangle)}{1 - \\sigma (\\langle x_i, w_{old}\\rangle)}x_i\\right] + \\frac{1}{C}w_{old} =$$\n",
    "\n",
    "$$=- \\frac{1}{n}\\left[\\sum_{i=1}^n y_i\\frac{e^{-\\langle x_i, w_{old}\\rangle}\\sigma^2(\\langle x_i, w_{old}\\rangle)}{\\sigma (\\langle x_i, w_{old}\\rangle)}x_i - (1 - y_i)\\frac{e^{-\\langle x_i, w_{old}\\rangle}\\sigma^2 (\\langle x_i, w_{old}\\rangle)}{1 - \\sigma (\\langle x_i, w_{old}\\rangle)}x_i\\right] + \\frac{1}{C}w_{old} =$$\n",
    "\n",
    "$$= - \\frac{1}{n}\\left[\\sum_{i=1}^n y_ie^{-\\langle x_i, w_{old}\\rangle}\\sigma(\\langle x_i, w_{old}\\rangle)x_i - (1 - y_i)\\frac{e^{-\\langle x_i, w_{old}\\rangle}\\sigma^2 (\\langle x_i, w_{old}\\rangle)}{1 - \\sigma (\\langle x_i, w_{old}\\rangle)}x_i\\right] + \\frac{1}{C}w_{old}$$\n",
    "\n",
    "Осталось выражение под знаком суммы привести к общему знаменателю и немного преобразовать.\n",
    "\n",
    "$$y_ie^{-\\langle x_i, w_{old}\\rangle}\\sigma(\\langle x_i, w_{old}\\rangle)x_i - (1 - y_i)\\frac{e^{-\\langle x_i, w_{old}\\rangle}\\sigma^2 (\\langle x_i, w_{old}\\rangle)}{1 - \\sigma (\\langle x_i, w_{old}\\rangle)}x_i = \\frac{e^{-\\langle x_i, w_{old}\\rangle}\\sigma(\\langle x_i, w_{old}\\rangle)(y_i - \\sigma(\\langle x_i, w_{old}\\rangle))}{1 - \\sigma(\\langle x_i, w_{old}\\rangle)}x_i =$$\n",
    "\n",
    "$$=\\frac{(1 - \\sigma(\\langle x_i, w_{old}\\rangle))(y_i - \\sigma(\\langle x_i, w_{old}\\rangle))}{1 - \\sigma(\\langle x_i, w_{old}\\rangle)}x_i = (y_i - \\sigma(\\langle x_i, w_{old}\\rangle))x_i$$\n",
    "\n",
    "Здесь мы воспользовались тем, что $ e^{-x}\\sigma(x) = \\frac{e^{-x}}{1 + e^{-x}} = \\frac{1 + e^{-x} - 1}{1 + e^{-x}} = 1 - \\frac{1}{1 + e^{-x}} = 1 - \\sigma(x)$\n",
    "\n",
    "Собрав всё вместе, получаем итоговую формулу.\n",
    "\n",
    "$$w_{new} = w_{old} - \\alpha_k\\left( \\frac{1}{n}\\left[ \\sum_{i=1}^n (\\sigma(\\langle x_i, w_{old}\\rangle) - y_i)x_i\\right] + \\frac{1}{C}w_{old}\\right)$$\n",
    "\n",
    "Как и в случае линейной регрессии градиент пропорционален ошибке на текущем шаге. Обе формулы можно объединить в одну следующим образом:\n",
    "\n",
    "$$w_{new} = w_{old} - \\alpha_k\\left( \\frac{k}{n}\\left[ \\sum_{i=1}^n (a_i - y_i)x_i\\right] + \\frac{1}{C}w_{old}\\right)$$, где $a_i = \\langle x_i, w_{old}\\rangle, k = 2$ для линейной регрессии; $a_i = \\sigma (\\langle x_i, w_{old}\\rangle), k = 1$ для логистической."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача 3. Точное решение линейной регрессии\n",
    "\n",
    "На лекции было показано, что точное решение линейной регрессии имеет вид $w = (X^TX)^{-1}X^TY $. \n",
    "* Покажите, что это действительно является точкой минимума в случае, если матрица X имеет строк не меньше, чем столбцов и имеет полный ранг. Подсказка: посчитайте Гессиан и покажите, что в этом случае он положительно определен. \n",
    "* Выпишите точное решение для модели с $L2$ регуляризацией. Как L2 регуляризация помогает с точным решением где матрица X имеет линейно зависимые признаки?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ваше решение здесь***\n",
    "\n",
    "Гессиан функции нескольких переменных - это матрица, элементами которой являются вторые частные производные функции по всем парам аргументов. Посчитаем $(i, j)$ элемент Гессиана.\n",
    "\n",
    "$$\\frac{\\partial^2 L}{\\partial w_j \\partial w_k} = \\sum_{i=1}^n2x_j^ix_k^i$$\n",
    "\n",
    "Из этой формулы получается, что Гессиан имеет вид $2X^TX$. Как можно заметить, $X^TX$ является матрицей Грамма, которая для линейно независимых векторов положительно определена. Умножая положительно определённую матрицу на положительное число, получаем положительно определённую матрицу. Следовательно, Гессиан функции потерь положительно определён во всех $w$.\n",
    "\n",
    "Посчитаем градиент функции потерь с $L_2$ регуляризацией.\n",
    "\n",
    "$$grad(L(w)) = grad((Xw - y)(Xw - y)^T + \\frac{1}{C}ww^T) = grad((Xw - y)(Xw - y)^T) + grad(\\frac{1}{C}ww^T) = 2X^TXw - 2X^Ty + \\frac{2}{C}w=$$\n",
    "\n",
    "$$=2((X^TX + \\frac{1}{C}E)w - X^Ty)$$\n",
    "\n",
    "Теперь приравниваем его к нулю.\n",
    "\n",
    "$$2((X^TX + \\frac{1}{C}E)w - X^Ty) = 0$$\n",
    "\n",
    "$$(X^TX + \\frac{1}{C}E)w = X^Ty$$\n",
    "\n",
    "$$w = (X^TX + \\frac{1}{C}E)^{-1}X^Ty$$\n",
    "\n",
    "Дополнительное слагаемое $\\frac{1}{C}E$ позволяет избавиться от линейных зависимостей среди столбцов матрицы $X^TX$, так как к диагональным элементам добавляется фиксированное число. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача 4.  Предсказываем вероятности.\n",
    "\n",
    "Когда говорят о логистической регрессии, произносят фразу, что она \"предсказывает вероятности положительного класса\". Давайте разберемся, что же за этим стоит. Посчитаем математическое ожидание функции потерь и проверим, что предсказание алгоритма, оптимизирующее это мат. ожидание, будет являться вероятностью положительного класса. \n",
    "\n",
    "И так, функция потерь на объекте $x_i$, который имеет метку $y_i \\in \\{0,1\\}$  для предсказания $a(x_i)$ равна:\n",
    "$$L(y_i, b) =-[y_i == 1] \\log a(x_i)  - [y_i == 0] \\log(1 - a(x_i)) $$\n",
    "\n",
    "Где $[]$ означает индикатор $-$ он равен единице, если значение внутри него истинно, иначе он равен нулю. Тогда мат. ожидание при условии конкретного $x_i$  по определение мат. ожидания дискретной случайной величины:\n",
    "$$E(L | x_i) = -p(y_i = 1 |x_i ) \\log a(x_i)  - p(y_i = 0 | x_i) \\log( 1 - a(x_i))$$\n",
    "* Докажите, что значение $a(x_i)$, минимизирующее данное мат. ожидание, в точности равно $p(y_i = 1 |x_i)$, то есть равно вероятности положительного класса.\n",
    "\n",
    "Подсказка: возможно, придется воспользоваться, что  $p(y_i = 1 | x_i) + p(y_i = 0 | x_i) = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ваше решение здесь***\n",
    "\n",
    "Найдём минимум мат. ожидания стандартным методом мат. анализа. Сначала рассчитаем первую производную по $a(x_i)$.\n",
    "\n",
    "$$\\frac{dE(L|x_i)}{da(x_i)} = -\\frac{p(y_i = 1|x_i)}{a(x_i)} + \\frac{p(y_i = 0|x_i)}{1 - a(x_i)}$$\n",
    "\n",
    "Приравняем её к нулю.\n",
    "\n",
    "$$-\\frac{p(y_i = 1|x_i)}{a(x_i)} + \\frac{p(y_i = 0|x_i)}{1 - a(x_i)} = 0$$\n",
    "\n",
    "$$a(x_i)p(y = 0|x_i) = (1 - a(x_i))p(y = 1|x_i)$$\n",
    "\n",
    "Заменим $p(y = 0|x_i) = 1 - p(y = 1|x_i)$.\n",
    "\n",
    "$$a(x_i)(1 - p(y = 1|x_i)) = (1 - a(x_i))p(y = 1|x_i)$$\n",
    "\n",
    "Из этого уравнения получаем, что $a(x_i) = p(y = 1|x_i)$, значит это точка экстремума.\n",
    "\n",
    "Теперь посчитаем вторую производную.\n",
    "\n",
    "$$\\frac{d^2E(L|x_i)}{d(a(x_i))^2} = \\frac{p(y = 1|x_i)}{a^2(x_i)} + \\frac{p(y = 0|x_i)}{(1 - a(x_i))^2}$$\n",
    "\n",
    "Подставим в неё найденное значение.\n",
    "\n",
    "$$\\frac{d^2E(L|x_i)}{d(a(x_i))^2}|_{p(y = 1|x_i)} = \\frac{p(y = 1|x_i)}{p^2(y = 1|x_i)} + \\frac{p(y = 0|x_i)}{(1 - p(y = 1|x_i))^2} = \\frac{1}{p(y = 1|x_i)} + \\frac{1}{p(y = 0|x_i)} > 0, т.к. p(y = 0|x_i) > 0, p(y = 1|x_i) > 0$$\n",
    "\n",
    "Мы видим, что значение второй производной в этой точке положительно, а следовательно, это точка минимума."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача 5.  Смысл регуляризации.\n",
    "\n",
    "Нужно ли в L1/L2 регуляризации использовать свободный член $w_0$ (который не умножается ни на какой признак)?\n",
    "\n",
    "Подсказка: подумайте, для чего мы вводим $w_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ваше решение здесь*** \n",
    "\n",
    "Свободный член $w_0$ служит для смещения гиперплоскости, поэтому тоже подлежит регуляризации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Реализация линейной модели (4 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Зачем нужны батчи?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как Вы могли заметить из теоретического введения, что в случае SGD, что в случа mini-batch GD,  на каждой итерации обновление весов  происходит только по небольшой части данных (1 пример в случае SGD, batch примеров в случае mini-batch). То есть для каждой итерации нам *** не нужна вся выборка***. Мы можем просто итерироваться по выборке, беря батч нужного размера (далее 1 объект тоже будем называть батчом).\n",
    "\n",
    "Легко заметить, что в этом случае нам не нужно загружать все данные в оперативную память, достаточно просто считать батч с диска, обновить веса, считать диска другой батч и так далее. В целях упрощения домашней работы, прямо с диска  мы считывать не будем, будем работать с обычными numpy array. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Немножко про генераторы в Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея считывания данных кусками удачно ложится на так называемые ***генераторы*** из языка Python. В данной работе Вам предлагается не только разобраться с логистической регрессией, но  и познакомиться с таким важным элементом языка.  При желании Вы можете убрать весь код, связанный с генераторами, и реализовать логистическую регрессию и без них, ***штрафоваться это никак не будет***. Главное, чтобы сама модель была реализована правильно, и все пункты были выполнены. \n",
    "\n",
    "Подробнее можно почитать вот тут https://anandology.com/python-practice-book/iterators.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К генератору стоит относиться просто как к функции, которая порождает не один объект, а целую последовательность объектов. Новое значение из последовательности генерируется с помощью ключевого слова ***yield***. Ниже Вы можете насладиться  генератором чисел Фибоначчи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def fib(max_iter=4):\n",
    "    a, b = 0, 1\n",
    "    iter_num = 0\n",
    "    while 1:\n",
    "        yield a\n",
    "        a, b = b, a + b\n",
    "        iter_num += 1\n",
    "        if iter_num == max_iter:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот так можно сгенерировать последовательность Фибоначчи. \n",
    "\n",
    "Заметьте, что к генераторам можно применять некоторые стандартные функции из Python, например enumerate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fib num: 0 fib values: 0\n",
      "Fib num: 1 fib values: 1\n",
      "Fib num: 2 fib values: 1\n",
      "Fib num: 3 fib values: 2\n"
     ]
    }
   ],
   "source": [
    "new_generator = fib()\n",
    "for j, fib_val in enumerate(new_generator):\n",
    "    print (\"Fib num: \" + str(j) + \" fib values: \" + str(fib_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пересоздавая объект, можно сколько угодно раз генерировать заново последовательность. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fib num: 0 fib values: 0\n",
      "Fib num: 1 fib values: 1\n",
      "Fib num: 2 fib values: 1\n",
      "Fib num: 3 fib values: 2\n",
      "Fib num: 0 fib values: 0\n",
      "Fib num: 1 fib values: 1\n",
      "Fib num: 2 fib values: 1\n",
      "Fib num: 3 fib values: 2\n",
      "Fib num: 0 fib values: 0\n",
      "Fib num: 1 fib values: 1\n",
      "Fib num: 2 fib values: 1\n",
      "Fib num: 3 fib values: 2\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 3):\n",
    "    new_generator = fib()\n",
    "    for j, fib_val in enumerate(new_generator):\n",
    "        print (\"Fib num: \" + str(j) + \" fib values: \" + str(fib_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А вот так уже нельзя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fib num: 0 fib values: 0\n",
      "Fib num: 1 fib values: 1\n",
      "Fib num: 2 fib values: 1\n",
      "Fib num: 3 fib values: 2\n"
     ]
    }
   ],
   "source": [
    "new_generator = fib()\n",
    "for i in range(0, 3):\n",
    "    for j, fib_val in enumerate(new_generator):\n",
    "        print (\"Fib num: \" + str(j) + \" fib values: \" + str(fib_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Концепция крайне удобная для обучения  моделей $-$ у Вас есть некий источник данных, который Вам выдает их кусками, и Вам совершенно все равно откуда он их берет. Под ним может скрывать как массив в оперативной памяти, как файл на жестком диске, так и SQL база данных. Вы сами данные никуда не сохраняете, оперативную память экономите."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если Вам понравилась идея с генераторами, то Вы можете реализовать свой, используя прототип batch_generator. В нем Вам нужно выдавать батчи признаков и ответов для каждой новой итерации спуска. Если не понравилась идея, то можете реализовывать SGD или mini-batch GD без генераторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, y, shuffle=True, batch_size=1):\n",
    "    \"\"\"\n",
    "    Гератор новых батчей для обучения\n",
    "    X          - матрица объекты-признаки\n",
    "    y          - вектор ответов\n",
    "    shuffle    - нужно ли случайно перемешивать выборку\n",
    "    batch_size - размер батча ( 1 это SGD, > 1 mini-batch GD)\n",
    "    Генерирует подвыборку для итерации спуска (X_batch, y_batch)\n",
    "    \"\"\"\n",
    "    size_ = y.shape[0]\n",
    "    indeces = np.arange(size_)\n",
    "    \n",
    "    if (shuffle):\n",
    "        indeces = np.random.permutation(indeces)\n",
    "    \n",
    "    #X_batch = \"\"\n",
    "    #y_batch = \"\"\n",
    "    \n",
    "    finish = 0\n",
    "    \n",
    "    for i in range(size_ // batch_size):\n",
    "        start = finish\n",
    "        finish = min(start + batch_size, size_)\n",
    "        X_batch = X[indeces[start:finish]]\n",
    "        y_batch = y[indeces[start:finish]]\n",
    "        yield (X_batch, y_batch)\n",
    "\n",
    "# Теперь можно сделать генератор по данным ()\n",
    "#  my_batch_generator = batch_generator(X, y, shuffle=True, batch_size=1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%pycodestyle\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Вычисляем значение сигмоида.\n",
    "    X - выход линейной модели\n",
    "    \"\"\"\n",
    "    \n",
    "    sigm_value_x = 1 / (1 + np.exp(-x))\n",
    "    return sigm_value_x\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class MySGDClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, batch_generator, C=1, alpha=0.01, max_epoch=10, model_type='lin_reg'):\n",
    "        \"\"\"\n",
    "        batch_generator -- функция генератор, которой будем создавать батчи\n",
    "        C - коэф. регуляризации\n",
    "        alpha - скорость спуска\n",
    "        max_epoch - максимальное количество эпох\n",
    "        model_type - тим модели, lin_reg или log_reg\n",
    "        \"\"\"\n",
    "        \n",
    "        self.C = C\n",
    "        self.alpha = alpha\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_generator = batch_generator\n",
    "        self.errors_log = {'iter' : [], 'loss' : []}  \n",
    "        self.model_type = model_type\n",
    "        \n",
    "    def calc_loss(self, X_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Считаем функцию потерь по батчу \n",
    "        X_batch - матрица объекты-признаки по батчу\n",
    "        y_batch - вектор ответов по батчу\n",
    "        Не забудте тип модели (линейная или логистическая регрессия)!\n",
    "        \"\"\"\n",
    "        \n",
    "        r = np.dot(X_batch, self.weights)\n",
    "        if self.model_type == 'log_reg':\n",
    "            r = sigmoid(r)\n",
    "        \n",
    "        if self.model_type == 'lin_reg':\n",
    "            loss = np.mean((y_batch - r) ** 2) + (np.linalg.norm(self.weights)**2)/2*self.C\n",
    "        elif self.model_type == 'log_reg':\n",
    "            loss = -np.mean(y_batch * np.log(r) + (1 - y_batch) * np.log(1 - r)) + (np.linalg.norm(self.weights)**2)/2*self.C\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    def calc_loss_grad(self, X_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Считаем  градиент функции потерь по батчу (то что Вы вывели в задании 1)\n",
    "        X_batch - матрица объекты-признаки по батчу\n",
    "        y_batch - вектор ответов по батчу\n",
    "        Не забудте тип модели (линейная или логистическая регрессия)!\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.model_type == 'lin_reg':\n",
    "            r = np.dot(X_batch, self.weights)\n",
    "            k = 2\n",
    "        elif self.model_type == 'log_reg':\n",
    "            r = np.dot(X_batch, self.weights)\n",
    "            r = sigmoid(r)\n",
    "            k = 1\n",
    "            \n",
    "        loss_grad = k * np.mean(X_batch * (r - y_batch), axis=0) + self.weights/self.C\n",
    "        \n",
    "        return loss_grad\n",
    "    \n",
    "    def update_weights(self, new_grad):\n",
    "        \"\"\"\n",
    "        Обновляем вектор весов\n",
    "        new_grad - градиент по батчу\n",
    "        \"\"\"\n",
    "        \n",
    "        self.weights = self.weights - self.alpha * new_grad\n",
    "    \n",
    "    def fit(self, X, y, batch_size=1):\n",
    "        '''\n",
    "        Обучение модели\n",
    "        X - матрица объекты-признаки\n",
    "        y - вектор ответов\n",
    "        '''\n",
    "        newX = np.hstack((np.ones(X.shape[0]).reshape(-1, 1), X))\n",
    "        newy = y\n",
    "        \n",
    "        self.weights = np.random.sample(X.shape[1] + 1)\n",
    "        for n in range(0, self.max_epoch):\n",
    "            new_epoch_generator = self.batch_generator(newX, newy, batch_size)\n",
    "            for batch_num, new_batch in enumerate(new_epoch_generator):\n",
    "                X_batch = new_batch[0]\n",
    "                y_batch = new_batch[1]\n",
    "                batch_grad = self.calc_loss_grad(X_batch, y_batch)\n",
    "                self.update_weights(batch_grad)\n",
    "                # Подумайте в каком месте стоит посчитать ошибку для отладки модели\n",
    "                # До градиентного шага или после\n",
    "                batch_loss = self.calc_loss(X_batch, y_batch)\n",
    "                self.errors_log['iter'].append(batch_num)\n",
    "                self.errors_log['loss'].append(batch_loss)\n",
    "                \n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Предсказание класса\n",
    "        X - матрица объекты-признаки\n",
    "        Не забудте тип модели (линейная или логистическая регрессия)!\n",
    "        '''\n",
    "        \n",
    "        r = np.dot(X, self.weights)\n",
    "        if self.model_type == 'log_reg':\n",
    "            r = sigmoid(r)\n",
    "        \n",
    "        y_hat = np.where(r >= 0, 1, 0)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустите обе регрессии на синтетических данных. \n",
    "\n",
    "\n",
    "Выведите полученные веса и нарисуйте разделяющую границу между классами (используйте только первых два веса для первых двух признаков X[:,0], X[:,1] для отображения в 2d пространство ).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(clf):\n",
    "    x1 = np.arange(-5, 10)\n",
    "    w0 = clf.weights[0]\n",
    "    w1 = clf.weights[1]\n",
    "    w2 = clf.weights[2]\n",
    "    plt.plot(x1, -w0/w2 - (w1/w2)*x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1fb9fa5e240>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "C1 = np.array([[0., -0.8], [1.5, 0.8]])\n",
    "C2 = np.array([[1., -0.7], [2., 0.7]])\n",
    "gauss1 = np.dot(np.random.randn(200, 2) + np.array([5, 3]), C1)\n",
    "gauss2 = np.dot(np.random.randn(200, 2) + np.array([1.5, 0]), C2)\n",
    "\n",
    "X = np.vstack([gauss1, gauss2])\n",
    "y = np.r_[np.ones(200), np.zeros(200)]\n",
    "\n",
    "my_clf1 = MySGDClassifier(batch_generator, model_type='lin_reg', max_epoch=10)\n",
    "my_clf1.fit(X, y, batch_size=2)\n",
    "\n",
    "my_clf2 = MySGDClassifier(batch_generator, model_type='log_reg', max_epoch=10)\n",
    "my_clf2.fit(X, y)\n",
    "\n",
    "plot_decision_boundary(my_clf1)\n",
    "plot_decision_boundary(my_clf2)\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее будем анализировать Ваш алгоритм. \n",
    "Для этих заданий используйте датасет ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=100000, n_features=10, \n",
    "                           n_informative=4, n_redundant=0, \n",
    "                           random_state=123, class_sep=1.0,\n",
    "                           n_clusters_per_class=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Покажите сходимости обеих регрессией на этом датасете: изобразите график  функции потерь, усредненной по $N$ шагам градиентого спуска, для разных `alpha` (размеров шага). Разные `alpha` расположите на одном графике. \n",
    "\n",
    "$N$ можно брать 10, 50, 100 и т.д. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "alphas = [0.01, 0.001, 0.0001]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "N = 100\n",
    "\n",
    "for alpha_ in alphas:\n",
    "    my_clf1 = MySGDClassifier(batch_generator, alpha=alpha_, model_type='lin_reg')\n",
    "    my_clf1.fit(X, y)\n",
    "    loss_values = np.array(my_clf1.errors_log['loss']).reshape(-1, N)\n",
    "    x = np.arange(loss_values.shape[0])\n",
    "    v = np.mean(loss_values, axis=1)\n",
    "    ax1.plot(x, v)\n",
    "    \n",
    "    my_clf2 = MySGDClassifier(batch_generator, alpha=alpha_, model_type='log_reg')\n",
    "    my_clf2.fit(X, y)\n",
    "    loss_values = np.array(my_clf2.errors_log['loss']).reshape(-1, N)\n",
    "    x = np.arange(loss_values.shape[0])\n",
    "    v = np.mean(loss_values, axis=1)\n",
    "    ax2.plot(x, v) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что Вы можете сказать про сходимость метода при различных `alpha`? Какое значение стоит выбирать для лучшей сходимости?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изобразите график среднего значения весов для обеих регрессий в зависимости от коеф. регуляризации С из `np.logspace(3, -3, 10)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Cs = np.logspace(3, -2, 10)\n",
    "\n",
    "mean_weights1 = []\n",
    "mean_weights2 = []\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "for C_ in Cs:\n",
    "    my_clf1 = MySGDClassifier(batch_generator, C=C_, model_type='lin_reg')\n",
    "    my_clf1.fit(X, y)\n",
    "    mean_weights1.append(np.mean(my_clf1.weights))\n",
    "    \n",
    "    my_clf2 = MySGDClassifier(batch_generator, C=C_, model_type='log_reg')\n",
    "    my_clf2.fit(X, y)\n",
    "    mean_weights2.append(np.mean(my_clf2.weights))\n",
    "    \n",
    "ax1.scatter(Cs, np.array(mean_weights1))\n",
    "ax2.scatter(Cs, np.array(mean_weights2))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Довольны ли Вы, насколько сильно уменьшились Ваши веса? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Боевое применение (4  балла)\n",
    "\n",
    "**Защита данной части возможна только при преодолении в проекте бейзлайна Handmade baseline.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте применим модель на итоговом проекте! Датасет сделаем точно таким же образом, как было показано в project_overview.ipynb\n",
    "\n",
    "Применим обе регрессии, подберем для них параметры и сравним качество. Может быть Вы еще одновременно с решением домашней работы подрастете на лидерборде!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28026\n"
     ]
    }
   ],
   "source": [
    "doc_to_title = {}\n",
    "with open('docs_titles.tsv') as f:\n",
    "    for num_line, line in enumerate(f):\n",
    "        if num_line == 0:\n",
    "            continue\n",
    "        data = line.strip().split('\\t', 1)\n",
    "        doc_id = int(data[0])\n",
    "        if len(data) == 1:\n",
    "            title = ''\n",
    "        else:\n",
    "            title = data[1]\n",
    "        doc_to_title[doc_id] = title\n",
    "print (len(doc_to_title))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv('train_groups.csv')\n",
    "traingroups_titledata = {}\n",
    "for i in range(len(train_data)):\n",
    "    new_doc = train_data.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    target = new_doc['target']\n",
    "    title = doc_to_title[doc_id]\n",
    "    if doc_group not in traingroups_titledata:\n",
    "        traingroups_titledata[doc_group] = []\n",
    "    traingroups_titledata[doc_group].append((doc_id, title, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 15) (11690,) (11690,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "y_train = []\n",
    "X_train = []\n",
    "groups_train = []\n",
    "for new_group in traingroups_titledata:\n",
    "    docs = traingroups_titledata[new_group]\n",
    "    for k, (doc_id, title, target_id) in enumerate(docs):\n",
    "        y_train.append(target_id)\n",
    "        groups_train.append(new_group)\n",
    "        all_dist = []\n",
    "        words = set(title.strip().split())\n",
    "        for j in range(0, len(docs)):\n",
    "            if k == j:\n",
    "                continue\n",
    "            doc_id_j, title_j, target_j = docs[j]\n",
    "            words_j = set(title_j.strip().split())\n",
    "            all_dist.append(len(words.intersection(words_j)))\n",
    "        X_train.append(sorted(all_dist, reverse=True)[0:15]    )\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "groups_train = np.array(groups_train)\n",
    "print (X_train.shape, y_train.shape, groups_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подберите размер батча для обучения. Линейная модель не должна учиться дольше нескольких минут. \n",
    "\n",
    "Не забывайте использовать скейлер!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбейте данные на обучение и валидацию. Подберите параметры C, alpha, max_epoch, model_type на валидации (Вы же помните, как правильно в этой задаче делать валидацию?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подберите порог линейной модели, по достижении которого, Вы будете относить объект к классу 1. Вспомните, какую метрику мы оптимизируем в соревновании.  Как тогда правильно подобрать порог?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С лучшими параметрами на валидации сделайте предсказание на тестовом множестве, отправьте его на проверку на платформу kaggle. Убедитесь, что Вы смогли побить public score первого бейзлайна."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** При сдаче домашки Вам необходимо кроме ссылки на ноутбук показать Ваш ник на kaggle, под которым Вы залили решение, которое побило Handmade baseline. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Фидбек (бесценно)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Какие аспекты обучения линейных моделей Вам показались непонятными? Какое место стоит дополнительно объяснить?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ваше ответ здесь***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Здесь Вы можете оставить отзыв о этой домашней работе или о всем курсе.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** ВАШ ОТЗЫВ ЗДЕСЬ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "402px",
    "width": "253px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
